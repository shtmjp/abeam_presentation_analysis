{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from firthlogist import FirthLogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(\n",
    "    n_sample: int, beta: np.ndarray, rng: np.random.Generator\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "    n_feature = len(beta)\n",
    "    x = rng.normal(size=(n_sample, n_feature))\n",
    "    y = rng.binomial(1, 1 / (1 + np.exp(-x @ beta)))\n",
    "    return x, y\n",
    "\n",
    "models = [  # (method_name, model_constructor, model_kwargs)\n",
    "    (\"Firth\", FirthLogisticRegression, {\"fit_intercept\": False, \"max_iter\": 1000}),\n",
    "    (\"normal\", LogisticRegression, {\n",
    "        \"fit_intercept\": False, \"max_iter\": 1000, \"penalty\": None\n",
    "        }\n",
    "     ),\n",
    "    (\"CV_L2\", LogisticRegressionCV, {\n",
    "        \"fit_intercept\": False, \"max_iter\": 1000, \"penalty\": \"l2\",\n",
    "        \"cv\": 5, \"Cs\": [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "        }),\n",
    "    (\"L2\", LogisticRegression, {\n",
    "        \"fit_intercept\": False, \"max_iter\": 1000, \"penalty\": \"l2\", \"C\": 10\n",
    "        }\n",
    "     ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "MC = 500\n",
    "n_sample = 30\n",
    "n_feature = 5\n",
    "\n",
    "beta = rng.uniform(-1, 1, size=n_feature)\n",
    "print(beta)\n",
    "\n",
    "method2coefs = {method_name: np.empty((MC, n_feature)) for method_name, _, _ in models}\n",
    "\n",
    "for i in tqdm(range(MC)):\n",
    "    for method_name, model_constructor, model_kwargs in models:\n",
    "        X, y = generate_sample(n_sample, beta, rng)\n",
    "        model = model_constructor(**model_kwargs)\n",
    "        model.fit(X, y)\n",
    "        method2coefs[method_name][i, :] = model.coef_.squeeze()\n",
    "\n",
    "# plot the bias\n",
    "fig, axs = plt.subplots(\n",
    "    len(method2coefs), n_feature,\n",
    "    figsize=(3*n_feature, 3*len(method2coefs))\n",
    "    )\n",
    "for i, method in enumerate(method2coefs.keys()):\n",
    "    bias = method2coefs[method] - beta\n",
    "    for j in range(n_feature):\n",
    "        sns.boxplot(bias[:, j], ax=axs[i, j])\n",
    "        axs[i, j].set_title(\n",
    "            f'{method}: {j+1}, \\\n",
    "            \\n mean: {bias[:, j].mean():.2f}, std: {bias[:, j].std():.2f}'\n",
    "            )\n",
    "        axs[i, j].axhline(0, color='r', linestyle='--')\n",
    "fig.suptitle(\n",
    "    f'Bias of Firth and Normal Logistic Regression \\n \\\n",
    "    (n_sample={n_sample}, n_feature={n_feature}), \\n \\\n",
    "    beta={np.round(beta, 3)}'\n",
    "    )\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
